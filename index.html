<!DOCTYPE HTML>
<html>
	<head>
		<title>Visual Question Answer</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">

					<!-- Logo -->
						<h2><a href="index.html" id="logo" >Visual Question Answering</a></h2>
						<h4>EECS 349, Machine Learning</h4>
						<h5>Xinke Yi, Xiangyun Zhao</h5>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li class="current"><a href="index.html">Home</a></li>
								<li><a href="dataset.html">Dataset</a></li>
								<li><a href="method.html">Method</a></li>
								<li><a href="result.html">Result</a></li>
								<li><a href="analysis.html">Analysis</a></li>
							</ul>
						</nav>

				</div>

			<!-- Banner -->
				<section id = "banner"></section>
				<!-- Gigantic Heading -->
				<section>
					<div class="container">
						<header class="major">
							<h2>Abstract of our project</h2>
							<h5>In this project, we do Visual Question Answering(VQA). Given an image and one question about that image, the VQA system aims to answer the question. Although humans can normally perform this task without major dificulties, to develop a system with these capabilities has always seemed closer to science fiction than to the current possibilities of Artificial Intelligence (AI). If such a system could be figured out, it could facilitate many important applications in our life such as Human Computer Interaction, Automated Driving System, and Artificial Robot. Problems combining image and language understanding such as image captioning and visual question answering (VQA) continue to inspire considerable research at the boundary of computer vision and natural language processing. </h5>
							<h5>In the Visual Question Answering, we formulate it as a classification problem. Each answer is assigned a class label. The VQA system is to predict the class label. So we first extract the image features and question representation, then fuse them to be a Visual Question representation. Finally, the Visual Question representation is forwarded through the classifier for answer prediction. We use a VGG network to extract the image features. A word embedding is learned to represent each word and an LSTM based question model is used to extract the question representation. We have tried two methods to fuse the image features and question representation: 1) Direct concatenation 2) Attention based fusion. Attention based fusion works better than direct concatenation. This illustrates the effectiveness of the attention mechanism in VQA. The improvement also verifies our assumption that VQA question is relevant to a region of the image and that image features should be highlighted. </h5>
							<span class="image formulation2"><img src="images/pic12.jpg" alt="" /><p>Overview of the framework we implemented. Given one image, the image features are extracted by CNN. Given one question, the question representation is extracted by LSTM based question model. Then the image features and question representation are fused, and the fused representation is forwarded though a classifier for answer prediction.</p></span>
						    
						</header>				
					</div>
				</section>

			<!-- Highlights -->
				<section class="wrapper style1">
					<div class="container">
						<div class="row gtr-200">
							<section class="col-4 col-12-narrower">
								<div class="box highlight">
									<i class="icon major fa-paper-plane"></i>
									<h3>Group Title</h3>
									<p>Visual Question Answering</p>
								</div>
							</section>
							<section class="col-4 col-12-narrower">
								<div class="box highlight">
									<i class="icon major fa-pencil"></i>
									<h3>Members</h3>
									<p>Xiangyun Zhao, Xinke Yi</p>
									<p>xiangyunzhao2022@u.northwestern.edu andy.yi@u.northwestern.edu</p>
								</div>
							</section>
							<section class="col-4 col-12-narrower">
								<div class="box highlight">
									<i class="icon major fa-wrench"></i>
									<h3>Course and University</h3>
									<p>EECS 349 Machine Learning, Northwestern University</p>
								</div>
							</section>
						</div>
					</div>
				</section>
			<!-- Footer -->
				<div id="footer">					
					<div class="copyright">
						<ul class="menu">
							<li>Â© Xiangyun Zhao, Xinke Yi</li><li><a href="https://woshiyixinke.github.io/">Northwestern University</a></li>
						</ul>
					</div>
				</div>
		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>